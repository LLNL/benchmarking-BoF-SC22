# benchmarking-BoF-SC22

## Abstract

HPC centers around the world use benchmarks to evaluate their machines and to engage with vendors during procurement.  The goal of this BoF is twofold. First, a series of short presentations will gather information on the state of the art methodologies for creating and validating the benchmarking sets. Second, an open discussion will  gather community feedback on pitfalls of the current methodologies and how these methodologies should evolve to accommodate the growing diversity of the computational workloads and HPC architectures.  The intended audience is HPC application developers and users, teams benchmarking HPC data centers, HPC vendors, and performance researchers.

## Panel

1.  Moderator: [Olga Pearce](https://people.llnl.gov/pearce8), LLNL, USA
2.  [Brian Austin](https://www.nersc.gov/about/nersc-staff/advanced-technologies-group/brian-austin), NERSC, USA
3.  [Jens Domke](https://domke.gitlab.io), RIKEN, Japan
4.  [Todd Gamblin](https://people.llnl.gov/tgamblin), LLNL, USA
5.  [Josef Weidendorfer](https://www.ce.cit.tum.de/en/caps/staff/josef-weidendorfer), TUM, Germany
6.  [Veronica Vergara](https://www.ornl.gov/staff-profile/veronica-g-melesse-vergara), ORNL, USA


## Please fill out our [SURVEY](https://docs.google.com/forms/d/e/1FAIpQLSearrW0ryZeGPwP4OM5hrv8WgjK86K9WN5jwDXG0BheGGTe7Q/viewform)


## Topics for discussion

### Benchmarking uses

- Hardware evaluation
- Procurement bids (e.g., CORAL2)
- Tracking progress
- Acceptance testing
 

### Challenges

- Many tedious manual tasks
- Build porting/portability across architectures
- Run script porting/portability across architectures
- Need to be run frequently (continuous task?)

###  Benchmark suite components

1.  Software
2.  Building
3.  Running
4.  Run specification for different machines and scales
5.  Output specification
6.  Performance evaluation
7.  CI and contiguous testing



## Notes from the meeting on 11/15/2022

### Olga
- Use them to evaluate hardware, acceptance, etc.
- El Capit√°n is coming in 2023
- Use internal harness to run tests (functionality, performance, stability)
- Benchmarks have been written and haven‚Äôt changed since 2018
- Lulesh has been retired because LLNL no longer uses those algorithms
- Can we do a little better with our benchmarking efforts?
 
### Jens
- Brief overview of the procurement for Fugaku
- Science areas that they wanted to advance with Fugaku
- Where do we use benchmarks?
- Different things to analyze benchmarks
- Hope is that we get help from the community to continue developing
 
### Brian
- NERSC Perlmutter supercomputer
- CPU & GPU partitions (separate)
- 600 different codes ran on Perlmutter
- For procurement, we choose application benchmarks
- NERSC-10 procurement will be released next summer
- Relevant and challenge problems with a 10-year outlook
- Perlmutter represents the next stage of the benchmark system
- Check that the system is healthy
- Defining the run rules, which changes to the benchmark a vendor may be allowed to make during the procurement process
 
### Josef
Octoberfest üòä
Difficult to enforce how much time they can spend on optimizations
Output of the tests are custom results
Challenges:
keeping benchmark suites up to date
still a lot of manual work to do
testbed software stacks are not mature enough
reproducibility

### Veronica
Vendor diagnostics, component-specific tests
Functionality tests
Performance tests: Running tests on a quiet system is important
Stability
[OLCF harness](https://github.com/olcf/olcf-test-harness)

### Todd
Historically, ran benchmarking the same way Ver√≥nica described
CORAL-2 procurement benchmarks
It is a giant pain to send tarballs around and it is hard to keep the benchmarks up to date
Use it for testing our own Linux distribution
Really common activity ‚Äì I don‚Äôt see why we couldn‚Äôt crowd-source
Building is one of those things that is fairly manual ‚Äì spack helps with that
Big community that continues to grow over time
The cloud is leveraged for CI
Right now, we use gitlab for CI to ensure a package can still build
If you can get engagement, I think you can crowd-source
ReFrame is getting some uptake
ReFrame you can plug easy-build or spack easily
Gaps:
Repository of recipes is hard
Benchmarks don‚Äôt have as broader reach as they could
Publicly shaming people is very effective
Incentivizes vendors to make improvements
 
### Q&A
Not a lot there on output or interpretation
There seems to be a lot of confusion ‚Äì who understands what benchmarks to what degree
What/who sets the ‚Äúpass‚Äù criteria?
Parametrizing can be painful
Correct answers, performance checks
Output standardization is very difficult, we‚Äôve seen everything from ‚ÄúOK‚Äù to 5GB of output and everything in between
Smoke test and then get the best performance
Two ways of thinking about it
But what is good enough?
Helps average John/Jane Doe user
Understanding what the performance was even on the cloud to see if you see a difference
Perfkit at Google
Now that I have a result, how do we compare to your results?
What is our collaboration forum look like?
Get the community to collaborate, try tools, not wait on consensus
Reproducible performance:
How would we containerize a benchmark so people can easily pull the container from the registry?
Containers won‚Äôt give you reproducible performance
A huge part of benchmarking is to show that a benchmark can hit the best performance after optimization
If you are running the flags for one site may be different than the other
Buy-in from library developers may help to broaden who provides the tests
Some library developers do provide tests already and would be willing to collaborate
Benchmark suites should be built by anyone that wants to build one
Maybe don‚Äôt think about them as benchmark tools
If you provide added value to scientists, they may be more willing to contribute
Having ways to demonstrate the user how to improve their applications to create a feedback loop
Before you know it, the full workflow will become the next step, but there is still a case for a more specialized tool
Capturing the environment and the state of the machine:
Is there a fingerprint per machine?
Best effort: whatever you can collect
