# benchmarking-BoF-SC22

## Abstract

HPC centers around the world use benchmarks to evaluate their machines and to engage with vendors during procurement.  The goal of this BoF is twofold. First, a series of short presentations will gather information on the state of the art methodologies for creating and validating the benchmarking sets. Second, an open discussion will  gather community feedback on pitfalls of the current methodologies and how these methodologies should evolve to accommodate the growing diversity of the computational workloads and HPC architectures.  The intended audience is HPC application developers and users, teams benchmarking HPC data centers, HPC vendors, and performance researchers.

## Panel

1.  Moderator: [Olga Pearce](https://people.llnl.gov/pearce8), LLNL, USA
2.  [Brian Austin](https://www.nersc.gov/about/nersc-staff/advanced-technologies-group/brian-austin), NERSC, USA
3.  [Jens Domke](https://domke.gitlab.io), RIKEN, Japan
4.  [Todd Gamblin](https://people.llnl.gov/tgamblin), LLNL, USA
5.  [Josef Weidendorfer](https://www.ce.cit.tum.de/en/caps/staff/josef-weidendorfer), TUM, Germany
6.  [Christopher Zimmer](https://www.olcf.ornl.gov/directory/staff-member/christopher-zimmer), ORNL, USA

## Topics for discussion

### Benchmarking uses

- Hardware evaluation
- Procurement bids (e.g., CORAL2)
- Systems acceptance testing

### Challenges

- Many tedious manual tasks
- Build porting/portability across architectures
- Run script porting/portability across architectures
- Need to be run frequently (continuous task?)

###  Benchmark suite components

1.  Software
2.  Build configurations
3.  Run description/inputs
4.  Run mechanism
